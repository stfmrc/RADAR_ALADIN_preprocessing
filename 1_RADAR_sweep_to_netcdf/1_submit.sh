#!/bin/bash
# -------------------------------------------------------------------
# Script: 1_submit.sh
# Purpose: Automates the submission of radar data preprocessing jobs 
#          in parallel, with throttling to limit the number of 
#          concurrent Python processes.
#
# Input:
#   - Text files containing lists of .h5 radar data files, organized
#     in directories under:
#       /scratch_ssd2/stefanelli/3DHNN_18_06_2025/
#           2_DATASETS_PREPROCESSING_FOR_3DHNN/
#           1_sweep_to_netcdf/0_FILES_DIR/{RADAR}/
#
# Processing:
#   - Iterates over configured RADARS, SWEEPS, YEARS, and MONTHS.
#   - For each combination, finds chunk files (e.g. {YYYY}{MM}_partXXX.txt).
#   - Submits a Python processing job for each chunk using:
#       1_RADARS_TO_NETCDF_single_nc_files_separated_by_sweeps_path_from_file.py
#   - Runs jobs in the background but throttles concurrency using MAX_JOBS.
#   - Logs standard output and errors for each job separately with timestamped filenames.
#
# Output:
#   - NetCDF files generated by the Python script (organized per radar/sweep).
#   - Log files stored under:
#       1_RADARS_TO_NETCDF_LOGS/{RADAR}/{SWEEP}/
#
# Usage:
#   Run with nohup to detach and capture logs:
#       nohup ./1_submit.sh > 1_submit.sh_output.log 2> 1_submit.sh_error.log &
#
# Notes:
#   - MAX_JOBS controls the number of parallel jobs (default 20).
#   - SLEEP_BETWEEN_CHECKS defines how often to check for free slots (default 10s).
#   - Each Python job processes one chunk file at a time.
#
# -------------------------------------------------------------------

# -------------------------------
# CONFIGURATION
# -------------------------------
RADARS=("PAZZ41") # "PAZZ42")      # Radars to process
DIR_BASE="/scratch_ssd2/stefanelli/3DHNN_18_06_2025/2_DATASETS_PREPROCESSING_FOR_3DHNN/1_sweep_to_netcdf/0_FILES_DIR" #Path to files with .h5 paths
LOG_DIR="1_RADARS_TO_NETCDF_LOGS"
SCRIPT="1_RADARS_TO_NETCDF_single_nc_files_separated_by_sweeps_path_from_file.py"
SWEEPS=(sweep_0)                # Add other sweeps if needed
YEARS=(2019 2020 2021 2022 2023)
MONTHS=(01 02 03 04 05 06 07 08 09 10 11 12)
MAX_JOBS=20                      # Maximum concurrent Python jobs
SLEEP_BETWEEN_CHECKS=10         # Seconds to wait before checking job slots again

# Timestamp for this submission
TS=$(date +%Y%m%d_%H%M%S)

mkdir -p "$LOG_DIR"

# -------------------------------
# FUNCTION: Wait until a job slot is free
# -------------------------------
wait_for_slot() {
    while [ "$(jobs -rp | wc -l)" -ge "$MAX_JOBS" ]; do
        sleep "$SLEEP_BETWEEN_CHECKS"
    done
}

# -------------------------------
# MAIN LOOP
# -------------------------------
for radar in "${RADARS[@]}"; do
    echo "Starting submissions for radar: $radar"
    
    mkdir -p "$LOG_DIR/$radar"
    dir="$DIR_BASE/$radar"

    for sweep in "${SWEEPS[@]}"; do
        mkdir -p "$LOG_DIR/$radar/$sweep"

        for year in "${YEARS[@]}"; do
            for month in "${MONTHS[@]}"; do
                i=0
                for filename in "$dir/${year}${month}_part"*; do
                    i=$((i+1))

                    # Build log filenames with timestamp and index
                    LOG_BASE="$LOG_DIR/$radar/$sweep/${radar}_${year}${month}_part_$(printf %03d $i)_${TS}"
                    OUT_LOG="${LOG_BASE}_output.log"
                    ERR_LOG="${LOG_BASE}_error.log"

                    # Wait for a free slot
                    wait_for_slot

                    # Launch Python script in background
                    nohup stdbuf -oL -eL python "$SCRIPT" \
                        --file="$filename" --radar="$radar" --year="$year" --month="$month" --sweep="$sweep" \
                        > "$OUT_LOG" 2> "$ERR_LOG" &

                    PID=$!
                    echo "$filename -> PID: $PID"
                done
            done
        done
    done
    echo "$radar DONE!!!"
done

# Wait for all background jobs to finish
wait
echo "All submissions finished at $(date)"
